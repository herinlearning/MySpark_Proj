=================================================================================================================================================================================================================================

-- SQOOP EXPORT PREP WORK
=================================================================================================================================================================================================================================


--Connect to oracle and create database for reporting database
--user:root, password:cloudera


oracle -u root -p
create database lcdbdata_rpt_db;
grant all on lcdbdata_rpt_db.* to lcdbdata;
flush privileges;
use lcdbdata_rpt_db;
create table departments as select * from lcdbdata_db.departments where 1=2;
exit;


=================================================================================================================================================================================================================================

-- SQOOP EXPORT
=================================================================================================================================================================================================================================

shortcut to remember -
sqoop_import connectivity + export-dir (of hdfs file) + "input" fields/lines terminated + update key/mode + num mappers (default 4)


--For certification change database name lcdbdata_rpt_db to lcdbdata_db
sqoop export --connect "jdbc:oracle://gw01367.am.hedani.net/lcdbdata_rpt_db" \
       --username lcdbdata \
       --password <hidden> \
       --table departments \
       --export-dir /user/hive/warehouse/lcdbdata_ods.db/departments \
       --input-fields-terminated-by '|' \
       --input-lines-terminated-by '\n' \
       --num-mappers 2 \
       --batch \
       --outdir java_files


//update key and update data

sqoop export --connect "jdbc:oracle://gw01367.am.hedani.net/lcdb" \
  --username lcdbdata \
  --password <hidden> \
  --table departments \
  --export-dir /user/lcd/sqoop_import/sqoop_import/departments_export \
  --batch \
  --outdir java_files \
  -m 1 \
  --update-key department_id \
  --update-mode allowinsert


sqoop export --connect "jdbc:oracle://gw01367.am.hedani.net/lcdb" \
  --username lcdbdata \
  --password <hidden> \
  --table departments_test \
  --export-dir /user/hive/warehouse/departments_test \
  --input-fields-terminated-by '\001' \
  --input-lines-terminated-by '\n' \
  --num-mappers 2 \
  --batch \
  --outdir java_files \
  --input-null-string nvl \
  --input-null-non-string -1


//During pure inserts - there can be failure due to PK , etc - there are absurd inserts and process fails
//To avoid it there is option called staging table which is created as intermediate table and if process fails all records are copied to staging but none is commited to target
//If export is clear, then staging table is truncated automatically


--staging-table <table_name>
--clear-staging-table



sqoop export --connect "jdbc:oracle://gw01367.am.hedani.net/lcdb" \
  --username lcdbdata \
  --password <hidden> \
  --table departments_test \
  --export-dir /user/hive/warehouse/departments_test \
  --input-fields-terminated-by '\001' \
  --input-lines-terminated-by '\n' \
  --input-null-string nvl \
  --input-null-non-string -1 \
  --staging-table departments_stage \
  --clear-staging-table \
  --num-mappers 2 \
  --batch \
  --outdir java_files







--Merge process begins
hadoop fs -mkdir /user/lcd/sqoop_import/sqoop_merge

=================================================================================================================================================================================================================================

